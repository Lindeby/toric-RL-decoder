# standard libraries
from copy import deepcopy

from .ReplayMemory import PrioritizedReplayMemory
from .util import Transition, Action
# pytorch
import torch.distributed as dist
from torch.multiprocessing import Process, SimpleQueue

class Distributed():
    def __init__(self, policy_net, target_net, optimizer, env, replay_memory="proportional"):

        self.env = env
        self.optimizer = optimizer

        self.policy_net = policy_net
        self.target_net = target_net

        if replay_memory == "proportional":
            self.replay_memory = PrioritizedReplayMemory(100) # TODO: temp size


    def train(train_steps, no_actors, learning_rate, epsilon, batch_size, policy_update):
        size = no_actors +1
        processes = []

        # Communication channels between processes
        weight_queue = SimpleQueue()
        transition_queue = SimpleQueue()
        
        args = {"no_actors":5, "train_steps":train_steps, "batch_size":batch_size,
                "optimizer":self.optimizer, "model": self.policy_net, "learning_rate":learning_rate,
                "policy_update":policy_update, "replay_memory":self.replay_memory}
        learner_process = Process(target=_init_process, args=(0, size, _learner, weight_queue,
                                                            transition_queue, args))
        learner_process.start()
        processes.append(learner_process)


        args = {"train_steps": train_steps, "max_actions_per_episode":0, "update_policy":policy_update
                "epsilon":epsilon, "size_local_buffer":50, "min_qubit_errors":0, "model":deepcopy(self.policy_net),
                "env":self.env}
        for rank in range(no_actors):
            actor_process = Process(target=_init_process, args=(rank+1, size, actor, weight_queue,
                                                                transition_queue, args))
            actor_process.start()
            processes.append(actor_process)

        for p in processes:
            p.join()

    def _init_process(rank, size, fn, wq, tq, args, backend='gloo'):
        """ Initialize the distributed environment. """
        os.environ['MASTER_ADDR'] = '127.0.0.2'
        os.environ['MASTER_PORT'] = '29501'
        dist.init_process_group(backend, rank=rank, world_size=size)
        fn(rank, size, wq, tq, args)


    def _learner(self, rank, world_size, weight_queue, transition_queue, args):
        """The learner in a distributed RL setting. Updates the network params, pushes
        new network params to actors. Additionally, this function collects the transitions
        in the queue from the actors and manages the replay buffer.
        """

        # init counter
        cnt_push_new_weights = 0

        replay_memory = args["replay_memory"]

        # set network to train mode
        model = args["model"]
        model.train()

        # define criterion and optimizer
        criterion = nn.MSELoss(reduction='none')
        if args["optimizer"] == 'RMSprop':
            optimizer = optim.RMSprop(model.parameters(), lr=args["learning_rate"])
        elif args["optimizer"] == 'Adam':    
            optimizer = optim.Adam(model.parameters(), lr=args["learning_rate"])

        # Broadcast initial weights to actors
        group = dist.new_group([x for x in range(args["world_size"])])
        weights = parameters_to_vector(model.parameters())
        dist.broadcast(tensor=weights, src=rank, group=group) 
        #-----------------------------------------
        # Fix below line
        #----------------------------------------
        # Wait until replay memory has enough transitions for one batch
        while len(replay_memory) < 16:
            if not transition_queue.empty():
                transition = transition_queue.get()
                replay_memory.save(transition) # Assuming memory entry generated by actor

        for t in range(train_steps):
            # TODO: Fix update of priorities
            self.experience_replay(criterion, optimizer, batch_size) # perform one train step

            # Get incomming transitions
            while not transition_queue.empty():
                transitions = replay_queue.get()
                replay_memory.save(transition) # Assuming memory entry generated by actor

            
            # self.replay_memory.cut() # TODO

            cnt_push_new_weights += 1
            if cnt_push_new_weights % update_actor_policy_freq == 0:
                weights = parameters_to_vector(model.parameters())
                for actor in range(world_size-1):
                    weight_queue.put([weights.detach()])

                cnt_push_new_weights = 0

            # periodically evaluate network

                
    def _actor(self, rank, world_size, weight_queue, transition_queue, args):     # Minimum numbers of eror generated for each episode 
            
            # set network to eval mode
            model = args["model"]
            model.eval()

            env = args["env"]

            # Init network params
            weights = torch.zeros(1)
            dist.broadcast(tensor=weights, src=0)
            vector_to_parameters(weights, model.parameters())
            
            # init counters
            steps_counter = 0
            update_counter = 1
            # iteration = 0

            # local buffer to store transitions before sending
            local_buffer = []   
            
            # main loop over training steps 
            for iteration in range(args["train_steps"]):
                steps_per_episode = 0
                env.reset()
                terminal_state = False
                
                # solve one episode
                while not terminal_state and steps_per_episode < args["max_actions_per_episodes"] 
                    steps_per_episode += 1
                    steps_counter += 1
                    
                    # ---------------------------------------------------
                    # Fix Below
                    # ---------------------------------------------------

                    # select action using epsilon greedy policy
                    action = self.select_action(number_of_actions=no_actions,
                                                epsilon=epsilon, 
                                                grid_shift=self.grid_shift)

                    state, reward, terminal_state, _ = env.step(action)


                    # generate memory entry
                    # TODO: move code from toricmodel for memory genreation, change what is nesesary 
                    perspective, action_memory, reward, next_perspective, terminal = self.toric.generate_memory_entry(
                        action, reward, self.grid_shift)  

                    # TODO: push to local buffer
                    # save transition in memory
                    self.memory.save(Transition(perspective, action_memory, reward, next_perspective, terminal), 10000) # max priority

                    # set target_net to policy_net
                    if not weight_queue.empty():
                        w = weight_queue.get()[0]
                        vector_to_parameters(w, model.parameters())

                    # TODO: check if transitions should be pushed



    def experience_replay(self, criterion, optimizer, batch_size, target_model, policy_model, replay_memory, device, discount_factor):
        policy_model.train()# self.policy_net.train()
        target_model.eval()# self.target_net.eval()

        def getBatches(batch_size):

            def toNetInput(batch, device):
                batch_input = np.stack(batch, axis=0) # not sure if it does anything
                # from np to tensor
                tensor = from_numpy(batch_input)
                tensor = tensor.type('torch.Tensor')
                return tensor.to(device)

            # get transitions and unpack them to minibatch
            transitions, weights, indices = replay_memory(batch_size, 0.4) # TODO: either self.replay memory or replay memory provided
            mini_batch = Transition(*zip(*transitions))

            # preprocess batch_input and batch_target_input for the network
            batch_state = toNetInput(mini_batch.state, self.device) # TODO: either self.device or device provided
            batch_next_state = toNetInput(mini_batch.next_state, self.device)

            # unpack action batch
            batch_actions = Action(*zip(*mini_batch.action))
            batch_actions = np.array(batch_actions.action) - 1
            batch_actions = torch.Tensor(batch_actions).long()
            batch_actions = batch_actions.to(self.device) 

            # preprocess batch_terminal and batch reward
            batch_terminal = convert_from_np_to_tensor(np.array(mini_batch.terminal)) 
            batch_terminal = batch_terminal.to(self.device)
            batch_reward = convert_from_np_to_tensor(np.array(mini_batch.reward))
            batch_reward = batch_reward.to(self.device)

            return batch_state, batch_actions, batch_reward, batch_next_state

            
        batch_state, batch_actions, batch_reward, batch_next_state = getBatches(batch_size)

        # compute policy net output
        policy_output = update_policy(batch_state)#self.policy_net(batch_state) # TODO: implement update policy
        policy_output = policy_output.gather(1, batch_actions.view(-1, 1)).squeeze(1)    

        # compute target network output 
        target_output = self.get_target_network_output(batch_next_state, batch_size)    # TODO: implement get_target_network_output
        target_output = target_output.to(device)#target_output.to(self.device)
        y = batch_reward + (batch_terminal * discount_factor * target_output) #batch_reward + (batch_terminal * self.discount_factor * target_output)
        # compute loss and update replay memory
        loss = self.get_loss(criterion, optimizer, y, policy_output, weights, indices)
        # backpropagate loss
        loss.backward()
        optimizer.step()

    