import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist

from torch.nn.utils import parameters_to_vector, vector_to_parameters


def learner(rank, world_size, weight_queue, transition_queue, args):
    """The learner in a distributed RL setting. Updates the network params, pushes
    new network params to actors. Additionally, this function collects the transitions
    in the queue from the actors and manages the replay buffer.
    """

    """ 
    args = {"no_actors",
            "train_steps",
            "batch_size",
            "optimizer",
            "policy_net",
            "target_net",
            "learning_rate",
            "device",
            "policy_update",
            "replay_memory",
            "discount_factor"}
    """

    device = args["device"]
    replay_memory = args["replay_memory"]

    def getBatches(batch_size):

        def toNetInput(batch, device):
            batch_input = np.stack(batch, axis=0) # not sure if it does anything
            # from np to tensor
            tensor = from_numpy(batch_input)
            tensor = tensor.type('torch.Tensor')
            return tensor.to(device)

        # get transitions and unpack them to minibatch
        transitions, weights, indices = replay_memory(batch_size, 0.4)
        mini_batch = Transition(*zip(*transitions))

        # preprocess batch_input and batch_target_input for the network
        batch_state = toNetInput(mini_batch.state, device)
        batch_next_state = toNetInput(mini_batch.next_state, device)

        # unpack action batch
        batch_actions = Action(*zip(*mini_batch.action))
        batch_actions = np.array(batch_actions.action) - 1
        batch_actions = torch.Tensor(batch_actions).long()
        batch_actions = batch_actions.to(device) 

        # preprocess batch_terminal and batch reward
        batch_terminal = convert_from_np_to_tensor(np.array(mini_batch.terminal)) 
        batch_terminal = batch_terminal.to(device)
        batch_reward = convert_from_np_to_tensor(np.array(mini_batch.reward))
        batch_reward = batch_reward.to(device)

        return batch_state, batch_actions, batch_reward, batch_next_state, weights, indices

    # init counter
    push_new_weights = 0

    policy_net = args["policy_net"]
    target_net = args["target_net"]
    discount_factor = args["discount_factor"]

    # define criterion and optimizer
    criterion = nn.MSELoss(reduction='none')
    if args["optimizer"] == 'RMSprop':
        optimizer = optim.RMSprop(policy_net.parameters(), lr=args["learning_rate"])
    elif args["optimizer"] == 'Adam':    
        optimizer = optim.Adam(policy_net.parameters(), lr=args["learning_rate"])
    

    # Push initial network params
    weights = parameters_to_vector(policy_net.parameters())
    for actor in range(world_size-1):
        weight_queue.put([weights.detach()])


    # Wait until replay memory has enough transitions for one batch
    while replay_memory.filled_size() < 16:
        if not transition_queue.empty():
            transition = transition_queue.get()
            replay_memory.save(transition) # Assuming memory entry generated by actor

    # Start training
    for t in range(train_steps):
        print("learner: traning step: ",t," / ",train_steps)
        # Move to learner
        batch_state, batch_actions, batch_reward, batch_next_state , weights, indices = getBatches(batch_size)

        policy_net.train()
        target_net.eval()

        # compute policy net output
        policy_output = policy_net(batch_state)
        policy_output = policy_output.gather(1, batch_actions.view(-1, 1)).squeeze(1)

        # compute target network output
        target_output = self.predict(target_net, batch_next_state, batch_size)
        target_output = target_output.to(device)
        
        # compute loss and update replay memory
        y = batch_reward + ((not batch_terminal) * discount_factor * target_output)
        loss = self.getLoss(criterion, optimizer, y, policy_output, weights, indices) # Note: Also updates priorites
        
        # backpropagate loss
        loss.backward()
        optimizer.step()

        # Get incomming transitions
        while not transition_queue.empty():
            transitions = replay_queue.get()
            replay_memory.save(transition) # Assuming memory entry generated by actor

        push_new_weights += 1
        if push_new_weights % args["policy_update"] == 0:
            weights = parameters_to_vector(policy_net.parameters())
            for actor in range(world_size-1):
                weight_queue.put([weights.detach()])

            push_new_weights = 0

        # periodically evaluate network


def getLoss(self, criterion, optimizer, y, output, weights, indices):
    loss = criterion(y, output)
    optimizer.zero_grad()
    # for prioritized experience replay
    if self.replay_memory == 'proportional': # TODO: self.replay_memory or provide arg
        tensor = from_numpy(np.array(weights))
        tensor = tensor.type('torch.Tensor')
        loss = tensor * loss.cpu() # TODO: Move to gpu
        priorities = torch.Tensor(loss, requires_grad=False)
        priorities = np.absolute(priorities.detach().numpy())
        self.memory.priority_update(indices, priorities)
    return loss.mean()

def predict(self, net, batch_state, batch_size):
    """
    Params
    -------
    action_index: If the q value of the performed action is requested, 
    provide the chosen action index
    """
    net.eval()

    # Create containers
    batch_output = np.zeros(batch_size)
    batch_perspectives = np.zeros(shape=(batch_size, 2, self.system_size, self.system_size)) # TODO: either self.system_size or provide arg
    batch_actions = np.zeros(batch_size)

    for i in range(batch_size):
        if (batch_state[i].cpu().sum().item() == 0):
            batch_perspectives[i,:,:,:] = np.zeros(shape=(2, self.system_size, self.system_size))
        else:
            # Generate perspectives
            perspectives = generatePerspective(self.grid_shift, self.system_size, batch_state[i].cpu()) # TODO: either self.grid_shift or provide arg
            perspectives = Perspective(*zip(*perspectives))
            perspectives = np.array(perspectives.perspective)
            perspectives = from_numpy(perspectives)
            perspectives = tensor.type('torch.Tensor')
            perspectives = perspectives.to(self.device) # TODO either self.device or provide arg

            # prediction
            with torch.no_grad():
                output = net(perspectives)
                q_values = np.array(output.cpu())
                row, col = np.where(q_values == np.max(q_values))
                batch_output[i] = q_values[row[0], col[0]]      

            #perspective = perspectives[row[0]]
            #perspective = np.array(perspective.cpu())
            #batch_perspectives[i,:,:,:] = perspective
            #batch_actions[i] = col[0]

    batch_output = convert_from_np_to_tensor(batch_output)
    #batch_perspectives = convert_from_np_to_tensor(batch_perspectives)
    return batch_output#, batch_perspectives, batch_actions
